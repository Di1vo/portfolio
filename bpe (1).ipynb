{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1890510b-2964-477d-ae51-af0b6d4fea28",
   "metadata": {
    "tags": []
   },
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f877859f-edd9-47b4-a6ec-e987991f8f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792482dd-fb9e-4024-8728-c316fa245b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5abd09-0115-4589-bea3-b325ac033e12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880057fc-455b-4a5e-87bf-60b1d9e34322",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('full-lenta-lemmas.csv', sep=';', index_col=0)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e14ce52-4e24-424d-8468-6e5c379c5968",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4135828b-eec0-410c-84e4-b7597e5f0997",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bd1b98-0fdc-4806-96c4-23d81c48a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmas'] = df['lemmas'].apply(lambda x: x[1:-1].split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f180eb0f-108f-4da8-a8aa-0b67439839ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_list_from_shit(lst):\n",
    "    \n",
    "    for i in range(len(lst)):\n",
    "        lst[i] = re.sub(r\"'\",'', lst[i])\n",
    "        lst[i] = lst[i].replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b75160f-fae8-439d-aeda-a73a7ceb1751",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(df['lemmas']))):\n",
    "    \n",
    "    clear_list_from_shit(df['lemmas'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed442915-6a3f-4aeb-98e4-b8caee0d6c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_lemmas_sum(x, y):\n",
    "    \n",
    "    return x[x['date'] == y]['lemmas'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30994dd-1aee-4f4f-a65d-87d4a11c3dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dates = df['date'].unique()\n",
    "df_days = pd.DataFrame(columns = {'date', 'day_lemmas'})\n",
    "\n",
    "for i in tqdm(range(len(dates))):\n",
    "    \n",
    "    text = day_lemmas_sum(df, dates[i])\n",
    "    new_row = {'date':dates[i], 'day_lemmas':text}\n",
    "    df_days= df_days.append(new_row, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a74852-9a83-43eb-8005-395a7929fd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbca574-b511-48a4-9245-60d4fb65e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_days['day_lemmas'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620b2b47-f0f0-4adc-bbb1-29344ec89bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['date'] == dates[10]]['lemmas'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bdb4ce-6246-4a4e-8923-a1909caf5d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_days['date'] = pd.to_datetime(df_days['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e85a76-21e1-40ce-ba23-90cd813e7bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "x = pd.to_datetime(df_days['date'].max()) - datetime.timedelta(days=365)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed56009-f96e-4b47-804e-c95b843c9bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_last_365_days = df_days[df_days['date'] > x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49677784-1a1f-4b76-b678-b83efd286c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_last_365_days.to_csv('df_last_365_days.csv', header=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0e76cc-e8b4-40ee-9af5-afc3b5fb0ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69666dea-414d-48c8-996a-d6b735fea60a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ef5e94-a569-45fe-9743-e73c10e8a2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13131c64-39e3-46ef-bbff-6187c2709660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "003399ae-7aac-45bf-8614-d3e648f1d9f8",
   "metadata": {},
   "source": [
    "# Загрузка и обработка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80eac07b-f5f6-4951-a47b-bb02860e8d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8159522f44d14bce9175e091e15073ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e6548c874d342c18912eb35ab4245af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/362 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv('df_last_365_days.csv', sep=';', index_col=0)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Делаем листы листами\n",
    "\n",
    "df['day_lemmas'] = df['day_lemmas'].apply(lambda x: x[1:-1].split(','))\n",
    "\n",
    "def clear_list_from_shit(lst):\n",
    "    \n",
    "    for i in range(len(lst)):\n",
    "        lst[i] = re.sub(r\"'\",'', lst[i])\n",
    "        lst[i] = lst[i].replace(' ', '')\n",
    "        \n",
    "for i in tqdm(range(len(df['day_lemmas']))):\n",
    "    clear_list_from_shit(df['day_lemmas'][i])\n",
    "\n",
    "    \n",
    "# Столбец с текстом STR\n",
    "    \n",
    "df['day_texts'] = \"\"\n",
    "\n",
    "for i in range(len(df)):\n",
    "    df['day_texts'][i] = \" \".join(df['day_lemmas'][i])\n",
    "    \n",
    "    \n",
    "# Соединяем по 10 токенов в предложения\n",
    "\n",
    "def stroka_to_predl_list(strka):\n",
    "    \n",
    "    hmch = round(len(strka) / 10)\n",
    "    output = []\n",
    "    x = 0\n",
    "    y = 10\n",
    "    \n",
    "    for i in range(hmch):\n",
    "        \n",
    "        ten_words = strka[x:y]\n",
    "        cnct = \" \".join(ten_words)\n",
    "        output.append([cnct])\n",
    "        x += 10\n",
    "        y += 10\n",
    "    \n",
    "    return output\n",
    "\n",
    "df['day_predls'] = \"\"\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    \n",
    "    predls = stroka_to_predl_list(df['day_lemmas'][i])\n",
    "    df['day_predls'][i] = predls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1c25e70-9ff8-4b4a-84b4-21ad7d267ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>day_lemmas</th>\n",
       "      <th>day_texts</th>\n",
       "      <th>day_predls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-13</td>\n",
       "      <td>[бедный, народ, предложить, доплачивать, жизнь...</td>\n",
       "      <td>бедный народ предложить доплачивать жизнь лес ...</td>\n",
       "      <td>[[бедный народ предложить доплачивать жизнь ле...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-14</td>\n",
       "      <td>[медведев, назвать, охота, ведьма, арест, медв...</td>\n",
       "      <td>медведев назвать охота ведьма арест медведчук ...</td>\n",
       "      <td>[[медведев назвать охота ведьма арест медведчу...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-15</td>\n",
       "      <td>[spacex, запустить, starship, орбита, вокруг, ...</td>\n",
       "      <td>spacex запустить starship орбита вокруг земля ...</td>\n",
       "      <td>[[spacex запустить starship орбита вокруг земл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-16</td>\n",
       "      <td>[хамас, снова, выпустить, ракета, сторона, изр...</td>\n",
       "      <td>хамас снова выпустить ракета сторона израиль п...</td>\n",
       "      <td>[[хамас снова выпустить ракета сторона израиль...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-17</td>\n",
       "      <td>[спч, призвать, посмертно, наградить, спасать,...</td>\n",
       "      <td>спч призвать посмертно наградить спасать ребён...</td>\n",
       "      <td>[[спч призвать посмертно наградить спасать реб...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>2022-05-08</td>\n",
       "      <td>[калининград, назвать, подрывать, сила, нато, ...</td>\n",
       "      <td>калининград назвать подрывать сила нато авиано...</td>\n",
       "      <td>[[калининград назвать подрывать сила нато авиа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>2022-05-09</td>\n",
       "      <td>[матвиенко, призвать, пресекать, распространен...</td>\n",
       "      <td>матвиенко призвать пресекать распространение н...</td>\n",
       "      <td>[[матвиенко призвать пресекать распространение...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>2022-05-10</td>\n",
       "      <td>[российский, дипломат, оборвать, эфир, sky, ne...</td>\n",
       "      <td>российский дипломат оборвать эфир sky news из-...</td>\n",
       "      <td>[[российский дипломат оборвать эфир sky news и...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>2022-05-11</td>\n",
       "      <td>[турин, стартовать, евровидение-, международны...</td>\n",
       "      <td>турин стартовать евровидение- международный пе...</td>\n",
       "      <td>[[турин стартовать евровидение- международный ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>2022-05-12</td>\n",
       "      <td>[посол, рассказать, бытовой, русофобия, сша, п...</td>\n",
       "      <td>посол рассказать бытовой русофобия сша посол р...</td>\n",
       "      <td>[[посол рассказать бытовой русофобия сша посол...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>362 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date                                         day_lemmas  \\\n",
       "0    2021-05-13  [бедный, народ, предложить, доплачивать, жизнь...   \n",
       "1    2021-05-14  [медведев, назвать, охота, ведьма, арест, медв...   \n",
       "2    2021-05-15  [spacex, запустить, starship, орбита, вокруг, ...   \n",
       "3    2021-05-16  [хамас, снова, выпустить, ракета, сторона, изр...   \n",
       "4    2021-05-17  [спч, призвать, посмертно, наградить, спасать,...   \n",
       "..          ...                                                ...   \n",
       "357  2022-05-08  [калининград, назвать, подрывать, сила, нато, ...   \n",
       "358  2022-05-09  [матвиенко, призвать, пресекать, распространен...   \n",
       "359  2022-05-10  [российский, дипломат, оборвать, эфир, sky, ne...   \n",
       "360  2022-05-11  [турин, стартовать, евровидение-, международны...   \n",
       "361  2022-05-12  [посол, рассказать, бытовой, русофобия, сша, п...   \n",
       "\n",
       "                                             day_texts  \\\n",
       "0    бедный народ предложить доплачивать жизнь лес ...   \n",
       "1    медведев назвать охота ведьма арест медведчук ...   \n",
       "2    spacex запустить starship орбита вокруг земля ...   \n",
       "3    хамас снова выпустить ракета сторона израиль п...   \n",
       "4    спч призвать посмертно наградить спасать ребён...   \n",
       "..                                                 ...   \n",
       "357  калининград назвать подрывать сила нато авиано...   \n",
       "358  матвиенко призвать пресекать распространение н...   \n",
       "359  российский дипломат оборвать эфир sky news из-...   \n",
       "360  турин стартовать евровидение- международный пе...   \n",
       "361  посол рассказать бытовой русофобия сша посол р...   \n",
       "\n",
       "                                            day_predls  \n",
       "0    [[бедный народ предложить доплачивать жизнь ле...  \n",
       "1    [[медведев назвать охота ведьма арест медведчу...  \n",
       "2    [[spacex запустить starship орбита вокруг земл...  \n",
       "3    [[хамас снова выпустить ракета сторона израиль...  \n",
       "4    [[спч призвать посмертно наградить спасать реб...  \n",
       "..                                                 ...  \n",
       "357  [[калининград назвать подрывать сила нато авиа...  \n",
       "358  [[матвиенко призвать пресекать распространение...  \n",
       "359  [[российский дипломат оборвать эфир sky news и...  \n",
       "360  [[турин стартовать евровидение- международный ...  \n",
       "361  [[посол рассказать бытовой русофобия сша посол...  \n",
       "\n",
       "[362 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2054c6-2246-403d-a09c-023acee279af",
   "metadata": {},
   "source": [
    "# Алгоритм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23214a27-0bcc-4249-bde9-1df920bee4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np \n",
    "from datetime import datetime\n",
    "import json\n",
    "from IPython.core.display import display, HTML\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "\n",
    "# Сам алгоритм \n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "\n",
    "def merge_vocab(pair: tuple, v_in: dict) -> dict:\n",
    "    \"\"\"Step 3. Merge all occurrences of the most frequent pair\"\"\"\n",
    "    \n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    \n",
    "    xx = '_'.join(pair)\n",
    "    \n",
    "    for word in v_in:\n",
    "        # replace most frequent pair in all vocabulary\n",
    "        w_out = p.sub(xx, word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "\n",
    "    return v_out\n",
    "\n",
    "def get_stats(vocab: dict) -> dict:\n",
    "    \"\"\"Step 2. Get counts of pairs of consecutive symbols\"\"\"\n",
    "\n",
    "    pairs = defaultdict(int)\n",
    "\n",
    "    for word, frequency in vocab.items():\n",
    "\n",
    "        symbols = word.split(' ')\n",
    "\n",
    "        symbols = [i for i in symbols if not '</w>' in i ]\n",
    "\n",
    "        #symbols = [i for i in symbols if i not in stopwords ]\n",
    "\n",
    "        # Counting up occurrences of pairs\n",
    "        for i in range(len(symbols) - 1):\n",
    "\n",
    "            k = (symbols[i], symbols[i + 1])\n",
    "\n",
    "            # Пропускаем шумные токены в обработке \n",
    "            if 'mentula' in ' '.join(k):\n",
    "                continue\n",
    "            \n",
    "            pairs[k] += frequency\n",
    "\n",
    "    return pairs\n",
    "\n",
    "def build_vocab(corpus: str) -> dict:\n",
    "    \"\"\"Step 1. Build vocab from text corpus\"\"\"\n",
    "\n",
    "    # Separate each char in word by space and add mark end of token\n",
    "    tokens = [\" \".join(word.split(' ')) + \" </w>\" for word in corpus]\n",
    "    \n",
    "    # Count frequency of tokens in corpus\n",
    "    vocab = Counter(tokens)  \n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "602c3a98-18c9-4840-b099-51de5599cd7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['бедный народ предложить доплачивать жизнь лес действовать мировой система возмещение'],\n",
       " ['нанести вред экология неэффективный наносить вред окружающий среда вывод прислать'],\n",
       " ['эксперт благотворительный организация сеть третий мир изменение сложиться ситуация частность'],\n",
       " ['предлагать начать платить народ бедный страна жизнь лес охрана флора'],\n",
       " ['фауна это сообщать би-би-си настоящий момент ведущий экономика мир ежегодно']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Что подаю на вход\n",
    "\n",
    "df['day_predls'][0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e981596-7a5d-4b3c-9a96-5daea1d830a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['day_predls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10cf5820-65dd-482b-88d0-62cf0d838654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-05-12'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['date'][361]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b566c4f6-f066-490d-bcf2-953cdae06630",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_day_data_chunk = df['day_predls'][361]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78583577-d4cf-4a41-a7e9-3eb23b19ef68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5e9daee62347e19daa6ce53d691583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/372 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_new_day_data_chunk = []\n",
    "\n",
    "for chunk in tqdm(new_day_data_chunk):\n",
    "\n",
    "    text_for_bpe = [i for i in chunk]\n",
    "\n",
    "    text_for_bpe = [i for i in text_for_bpe if len(i.split(' ')) > 2]\n",
    "\n",
    "    # Свернем с сохранением порядка \n",
    "    text_for_bpe = list(dict.fromkeys(text_for_bpe).keys())\n",
    "\n",
    "    text = text_for_bpe\n",
    "    \n",
    "    vocab = build_vocab(text)\n",
    "\n",
    "    num_merges = 200  # Hyperparameter\n",
    "\n",
    "    out = []\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "\n",
    "            pairs = get_stats(vocab)  # Step 2\n",
    "\n",
    "            if not pairs:\n",
    "                break\n",
    "\n",
    "            best = max(pairs, key=pairs.get)\n",
    "\n",
    "            #print(best, pairs[best])\n",
    "\n",
    "            out += [(best, pairs[best])]\n",
    "\n",
    "            vocab = merge_vocab(best, vocab)\n",
    "            \n",
    "    out = [(' '.join(i[0]).replace('_', ' '), i[1]) for i in out]\n",
    "    new_new_day_data_chunk.append([out , chunk]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4495d98-0818-4e13-b295-c1358ea514a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "372"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_new_day_data_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06c2b4cc-2663-4fec-9474-1eb4373c803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "days = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82c0e131-171d-44f4-b6dd-7f24557f62b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "days['word'] = \"\"\n",
    "days['freq'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc2cd936-ce4d-4c0b-9b28-bcdca21e3a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(new_new_day_data_chunk)):\n",
    "    \n",
    "    new_row = {'word':new_new_day_data_chunk[i][0][0][0], 'freq':new_new_day_data_chunk[i][0][0][1]}\n",
    "    days= days.append(new_row, ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f607fc23-5d22-4148-ae55-02df870bf82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>президент бразилия</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>турция усомниться</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>кубок италия</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>долговой бремя</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>фамилия хансный</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>вне украина</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>россия wp</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>разрезать свадебный</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>жених напугать</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>гостья свадьба</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    word freq\n",
       "303   президент бразилия    2\n",
       "273    турция усомниться    2\n",
       "42          кубок италия    2\n",
       "222       долговой бремя    2\n",
       "245      фамилия хансный    1\n",
       "253          вне украина    1\n",
       "252            россия wp    1\n",
       "251  разрезать свадебный    1\n",
       "250       жених напугать    1\n",
       "249       гостья свадьба    1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days.sort_values(by='freq', ascending=False).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5a8a07-f354-4ddc-897c-66796320bb08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1079f41f-28bc-4efb-a1c2-9acd2f6db242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992a8abe-adb5-450f-bb53-51f5a62b0052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa6863-f47b-4713-a5d4-0acb6fca0385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e09409f-c966-4b63-a44c-4716aa46f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "def build_vocab(corpus: str) -> dict:\n",
    "    \"\"\"Step 1. Build vocab from text corpus\"\"\"\n",
    "\n",
    "    # Separate each char in word by space and add mark end of token\n",
    "    tokens = [\" \".join(word) + \" </w>\" for word in corpus.split()]\n",
    "    \n",
    "    # Count frequency of tokens in corpus\n",
    "    vocab = Counter(tokens)  \n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def get_stats(vocab: dict) -> dict:\n",
    "    \"\"\"Step 2. Get counts of pairs of consecutive symbols\"\"\"\n",
    "\n",
    "    pairs = defaultdict(int)\n",
    "    for word, frequency in vocab.items():\n",
    "        symbols = word.split()\n",
    "\n",
    "        # Counting up occurrences of pairs\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i + 1]] += frequency\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def merge_vocab(pair: tuple, v_in: dict) -> dict:\n",
    "    \"\"\"Step 3. Merge all occurrences of the most frequent pair\"\"\"\n",
    "    \n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    \n",
    "    for word in v_in:\n",
    "        # replace most frequent pair in all vocabulary\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "\n",
    "    return v_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d93c72-7ca8-4327-b287-f058ba413f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(df['day_texts'][0])  # Step 1\n",
    "\n",
    "num_merges = 500  # Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a28b47a-2758-4b36-bc12-1f76c272df5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_merges):\n",
    "\n",
    "    pairs = get_stats(vocab)  # Step 2\n",
    "\n",
    "    if not pairs:\n",
    "        break\n",
    "\n",
    "    # step 3\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2151ba7e-d9a5-4dee-9b40-2589f4ab6dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f42ba5-0b3d-42ed-8081-ef0834c9c0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(vocab.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aba339-24fc-4158-a1b8-43e705ae5995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
